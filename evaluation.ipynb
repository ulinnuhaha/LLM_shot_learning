{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "import json\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import os\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "bleu_score = evaluate.load(\"bleu\")\n",
    "chrf_score = evaluate.load(\"chrf\")\n",
    "sacrebleu_score = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to calculate evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(predictions, references):\n",
    "    # Ensure both predictions and references are lists of strings\n",
    "    predictions = [str(pred) if pred is not None else \"\" for pred in predictions]\n",
    "    references = [str(ref) if ref is not None else \"\" for ref in references]\n",
    "\n",
    "    # Compute scores\n",
    "    result = rouge_score.compute(predictions=predictions, references=references, rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "    score = sacrebleu_score.compute(\n",
    "            predictions=predictions,\n",
    "            references=references\n",
    "        )\n",
    "    result[\"sacrebleu\"] = score[\"score\"]\n",
    "    bleu = bleu_score.compute(predictions=predictions, references=references)\n",
    "    result[\"bleu\"] = bleu['bleu']\n",
    "    chrf = chrf_score.compute(predictions=predictions, references=references) ##The higher the value, the better the translations\n",
    "    chrf_plus = chrf_score.compute(predictions=predictions, references=references, word_order=2)  # chrF++\n",
    "    result[\"chrf++\"] = chrf_plus[\"score\"]\n",
    "    result[\"chrf\"] = chrf[\"score\"] #The higher the value, the better the translations\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter for performing Evaluations\n",
    "test_data = 'test_1'# test_2 / test_3\n",
    "llm_model = 'llama_31_70b' # gpt/ mt5\n",
    "batch_size = 25\n",
    "path_f = 'italian2ladin' # You can edit\n",
    "target_lang = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\n",
    "    \"Here are the translations of the 15 entries in the JSON format with the 'Italian' fields filled in:\",\n",
    "    \"Here are the translations:\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_files(llm_model, test_data):\n",
    "    # Get JSON files for specific translation test data using a specific llm\n",
    "    # Define the file prefix file name\n",
    "    file_prefix = (f'translation_{llm_model}_to_{target_lang}_ita2lad_size of_{batch_size}_batch_')\n",
    "\n",
    "    # List all files in the directory that start with the specified prefix\n",
    "    save_dir = 'save_results'\n",
    "    matching_files = [f for f in os.listdir(f'{save_dir}/{path_f}') if f.startswith(file_prefix)] #current_path+'/save_results'\n",
    "    # Count the number of matching files\n",
    "    num_files = len(matching_files)\n",
    "    print(f\"Found {num_files} files.\")\n",
    "\n",
    "    scores = {}\n",
    "    scores['rouge1'] = []\n",
    "    scores['rouge2'] = []\n",
    "    scores['rougeL'] = []\n",
    "    scores['bleu'] = []\n",
    "    scores['chrf'] = []\n",
    "    scores['sacrebleu'] = []\n",
    "    scores['chrf++'] = []\n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': [], 'bleu': [], 'chrf': [], 'sacrebleu': [], 'chrf++': []}\n",
    "    all_scores = []\n",
    "    batch_start = 0\n",
    "\n",
    "    # Open real data / our ground truth\n",
    "    # ----- Please Modify -----\n",
    "    ref_data = pd.read_csv(f'dataset/{test_data}.csv') \n",
    "    \n",
    "    for i in range(num_files):\n",
    "        # Slicing for the current batch of data\n",
    "        real_data = ref_data.iloc[batch_start:batch_start + batch_size]\n",
    "        batch_start = (i + 1) * batch_size\n",
    "        print(f\"Processing batch {i+1}, starting at index {batch_start}\")\n",
    "\n",
    "        # Get the real data as a list\n",
    "        ########### Set the Target Language #######################\n",
    "        real_data = real_data[target_lang].tolist() # ladin / italian ## DOnt forget to set the target language\n",
    "        ###########################################################\n",
    "\n",
    "        # Open and read the JSON files of translation result\n",
    "        file_loc=os.path.join(save_dir, path_f+f'/{file_prefix}{i}.json') #save_dir\n",
    "        print(\"load the json file\", file_loc)\n",
    "        f = open(file_loc, encoding='utf8')\n",
    "        data = json.load(f)\n",
    "\n",
    "        # Get the target translation using llm API\n",
    "        # if json data is in str, convert to dict\n",
    "        if isinstance(data, str):\n",
    "            data = json.loads(data)\n",
    "        # Ensure 'choices' exists and contains data\n",
    "        if \"choices\" in data and data[\"choices\"]:\n",
    "            translation_output = data[\"choices\"][0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            #print(translation_output)\n",
    "            if translation_output.strip():  # Check if translation output is not empty\n",
    "                try:\n",
    "                    if isinstance(translation_output, str):\n",
    "                        translation_output = translation_output.strip('```json\\n').strip('```')  # Clear unnecessary chars from GPT output\n",
    "\n",
    "                    # Remove the additional response\n",
    "                    for prefix in prefixes:\n",
    "                        if translation_output.startswith(prefix):\n",
    "                            translation_output = translation_output[len(prefix):].strip()\n",
    "                        \n",
    "                    \n",
    "                    translation_output = json.loads(translation_output)  # Parse the JSON output\n",
    "                    \n",
    "                    prediction = []\n",
    "                    # parsing the ladin and italian pair translation\n",
    "                    for translation_pair in translation_output.get('translations', []):\n",
    "                    # DOnt forget to set the target language\n",
    "                    ################################################\n",
    "                        result_translation = translation_pair.get(target_lang) \n",
    "                        prediction.append(result_translation) # \n",
    "                    ################################################\n",
    "\n",
    "                    # Calculate the evaluation metrics\n",
    "                    if len(prediction) == len(real_data):\n",
    "                        scores =eval_metrics(prediction, real_data)\n",
    "                    else:\n",
    "                        print('the sentences do not match')         \n",
    "                    # Append the pair to the combined_translations list\n",
    "                    all_scores.append(\n",
    "                                {'rouge1': scores['rouge1'],\n",
    "                                'rouge2':scores['rouge2'],\n",
    "                                'rougeL': scores['rougeL'],\n",
    "                                'bleu': scores['bleu'],\n",
    "                                'sacrebleu': scores['sacrebleu'],\n",
    "                                'chrf': scores['chrf'],\n",
    "                                'chrf++': scores['chrf++']}\n",
    "                                )\n",
    "                        \n",
    "                except json.JSONDecodeError as ex:\n",
    "                    #ex.with_traceback()\n",
    "                    # Your escaped JSON string\n",
    "                    print(f\"Error parsing JSON content in file: {file_loc}\")\n",
    "                    print(\"length of characters is\",len(translation_output))\n",
    "                    print(f\"Raw content causing the error:\\n{translation_output}\")\n",
    "        else:\n",
    "            print(\"No choices found in the response.\")\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all translation results\n",
    "translation_result=get_json_files(llm_model, test_data)\n",
    "print(len(translation_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a DataFrame\n",
    "filtered_data = [entry for entry in translation_result if not all(isinstance(v, list) and len(v) == 0 for v in entry.values())]\n",
    "fr = pd.DataFrame(filtered_data)\n",
    "print(len(filtered_data))\n",
    "# Calculate the mean for each column\n",
    "mean_scores = fr.mean()\n",
    "# Print the mean scores\n",
    "print(mean_scores)\n",
    "llm_model, test_data, path_f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
